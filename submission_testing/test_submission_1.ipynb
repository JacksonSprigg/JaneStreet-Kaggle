{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Set Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "### Environment Setup ###\n",
    "IS_KAGGLE = False  # Flag to switch between environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Dependent Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths based on environment\n",
    "if IS_KAGGLE:\n",
    "    data_path = '/kaggle/input/jane-street-real-time-market-data-forecasting'\n",
    "    trained_models_path = \"/kaggle/input/janestreet-models/trained_models/\"\n",
    "else:\n",
    "    # For local testing\n",
    "    import sys\n",
    "\n",
    "    # Path to main project directory\n",
    "    PROJ_DIR = os.path.dirname(os.getcwd())\n",
    "    \n",
    "    # Add paths to system path\n",
    "    sys.path.append(os.path.join(PROJ_DIR, \"jane-street-real-time-market-data-forecasting\"))\n",
    "    sys.path.append(os.path.join(PROJ_DIR, \"training\", \"src\", \"utils\"))\n",
    "    \n",
    "    # Set data path to our local test data\n",
    "    data_path = os.path.join(os.getcwd(), \"local_test_data\")\n",
    "    \n",
    "    # Import local testing metrics\n",
    "    from metrics import r2_score_weighted\n",
    "\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My models\n",
    "# \"val_r2_0.0122524.joblib\"\n",
    "# \"backup_best_valr2.joblib\"\n",
    "\n",
    "# Benchmark Models\n",
    "# LGBMV1_1.joblib\n",
    "\n",
    "model_name = \"val_r2_0.0122524.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prediction():\n",
    "    \"\"\"Load and setup model and preprocessing\"\"\"\n",
    "    if IS_KAGGLE:\n",
    "        model_path = trained_models_path + model_name\n",
    "    else:\n",
    "        model_path = os.path.join(PROJ_DIR, \"training\", \"trained_models\", model_name)\n",
    "    \n",
    "    # Load model directly as a Booster\n",
    "    model = lightgbm.Booster(model_file=model_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model globally\n",
    "model = setup_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FLoat value in predictions?\n",
    "\n",
    "# Global variables\n",
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    \n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    # Get predictions from model using all required features\n",
    "    feature_cols = ['symbol_id', 'weight'] + [f'feature_{i:02d}' for i in range(79)]\n",
    "    test_data = test.select(feature_cols).to_numpy()\n",
    "    model_predictions = model.predict(test_data)\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    predictions = test.select(\n",
    "        'row_id'\n",
    "    ).with_columns([\n",
    "        pl.Series(model_predictions, dtype=pl.Float64).alias('responder_6')\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(predictions, pl.DataFrame | pd.DataFrame) # The predict function must return a DataFrame\n",
    "    assert predictions.columns == ['row_id', 'responder_6']     # with columns 'row_id', 'responer_6'\n",
    "    assert len(predictions) == len(test)                        # and as many rows as the test data.\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729837785.434293 1149040 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring submission...\n",
      "Overall R² score: 0.00619853601475151\n"
     ]
    }
   ],
   "source": [
    "# Set up inference server\n",
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "# Run based on environment\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            os.path.join(data_path, \"test.parquet\"),\n",
    "            os.path.join(data_path, \"lags.parquet\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Local scoring\n",
    "    print(\"Scoring submission...\")\n",
    "    predictions = pl.read_parquet(\"submission.parquet\")                      # Read submission predictions\n",
    "    test_data = pl.read_parquet(os.path.join(data_path, \"test.parquet\"))     # Read test data with actual values\n",
    "\n",
    "    # Score only rows marked for scoring\n",
    "    mask = test_data['is_scored']\n",
    "    score = r2_score_weighted(\n",
    "        test_data.filter(mask)['responder_6'].to_numpy(),\n",
    "        predictions.filter(mask)['responder_6'].to_numpy(),\n",
    "        test_data.filter(mask)['weight'].to_numpy()\n",
    "    )\n",
    "    print(f\"Overall R² score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simxrd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
